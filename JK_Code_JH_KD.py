# -*- coding: utf-8 -*-
"""lyft

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p7FLrfuYoxYjAYW6ScCw8B7pr_5iALvj
"""

#setup
import pandas as pd
import numpy as np
from google.colab import drive
drive.mount('/content/drive')
import datetime
from datetime import timedelta  
from IPython.display import clear_output
import timeit
import matplotlib.pyplot as plt
import seaborn as sns
import math

driver_ids = pd.read_csv("/content/drive/My Drive/Lyft Data Challenge 2019/driver_ids.csv")
ride_ids = pd.read_csv("/content/drive/My Drive/Lyft Data Challenge 2019/ride_ids.csv")
ride_timestamps = pd.read_csv("/content/drive/My Drive/Lyft Data Challenge 2019/ride_timestamps.csv")

"""#figure out a good timeframe for calculating churn"""

#assuming that all completed rides have an associated drop-off timestamp, make new dataframe with only drop-off timestamps
ride_timestamps['timestamp'] =  pd.to_datetime(ride_timestamps['timestamp'])
ride_timestamps = ride_timestamps.set_index('ride_id')
dropped_off = ride_timestamps.loc[ride_timestamps['event'] == 'dropped_off_at']
dropped_off = dropped_off.reset_index()
dropped_off.drop_duplicates(subset='ride_id', keep='last')

#join drop-off timestamps with ride_ids dataframe > match each driver with a list of all his/her drop-off times
ride_ids = ride_ids.set_index('ride_id')
rides = ride_ids.merge(dropped_off, how='inner', left_index = True, right_on = 'ride_id')
rides = rides.reset_index().set_index('driver_id')
rides = rides.groupby('driver_id').agg({'timestamp':lambda x: list(x)})
rides = rides.reset_index()

#find time elapsed from one ride to the next for each driver
averages = []
differences = []

for i in rides.index:
  total = datetime.timedelta(seconds=0)
  timestamps = rides.at[i, 'timestamp']
  timestamps = sorted(timestamps)
  for j in range(len(timestamps)-1):
    difference = timestamps[j+1] - timestamps[j]
    differences.append(difference)
    total += difference
   
  average = total / len(timestamps)
  averages.append(average)

#averages timedeltas for each driver
averages = pd.DataFrame(averages)
avg_days = averages/pd.Timedelta(days=1)
avg_days = averages.sort_values(by=[0])
averages.describe()

#all timedeltas between rides given
differences = pd.DataFrame(differences)
differences_days = differences/pd.Timedelta(days=1)
differences_days.describe()

"""#find active vs inactive weeks, "true" lifetimes, and time from onboarding-first ride"""

#finds the first and last ride given by the driver + finds active weeks vs. inactive weeks
first_rides = []
last_rides = []
weeks_active = []
ids = []

for i in rides.index:
  timestamps = rides.at[i, 'timestamp']
  timestamps = sorted(timestamps)

  first_rides.append(timestamps[0])
  last_rides.append(timestamps[-1])
  first_ride = timestamps[0]
  last_ride = timestamps[-1]

  days_elapsed = last_ride-first_ride
  weeks_elapsed = math.ceil(days_elapsed/pd.Timedelta(days=7))
  start = first_ride
  active_weeks = 0

  for week in range(1,weeks_elapsed+1):
    eow = timestamps[0] + (week * pd.Timedelta(days=7))
    for timestamp in timestamps:
      if (start <= timestamp < eow):
        active_weeks += 1
        start = eow
        break 
  ids.append(rides.at[i,'driver_id'])
  weeks_active.append(active_weeks)

rides['first_ride'] = first_rides
rides['last_ride'] = last_rides

active_weeks = pd.DataFrame({'driver_id':ids,'active_weeks':weeks_active})

#finds the lifespans thus far of drivers that gave their first ride prior to april 4th (max lifespan ~ 13wks)
true_lifespans = []
ids = []
index = 0
for last_ride in last_rides:
  if first_rides[index] < datetime.datetime(2016,4,4):
    true_lifespan = last_ride - first_rides[index]
    true_lifespans.append(true_lifespan)
  else:
    true_lifespans.append(None)
  ids.append(rides.loc[index,'driver_id'])
  index += 1

true_lifespans = pd.DataFrame({'driver_id':ids,'mar_lifespans':true_lifespans})
true_lifespans.mar_lifespans = true_lifespans.mar_lifespans.apply(lambda x: x/pd.Timedelta(days=7))

#finds the time elapsed from onboarding to first ride given
rides = rides.merge(driver_ids, how = 'left', on= 'driver_id')
rides['driver_onboard_date'] = pd.to_datetime(rides['driver_onboard_date'])
rides['onboarding-first'] = rides.first_ride-rides.driver_onboard_date

"""#cohort churn analysis"""

#create cohorts by separating drivers into groups based on when the first ride was given
cohort1 = []
cohort2 = []
cohort3 = []
cohort4 = []
cohort5 = []
cohort6 = []
cohort7 = []
cohort8 = []
cohort9 = []

for ride in rides.index:
  if rides.loc[ride, 'first_ride'] >= datetime.datetime(2016,3,28) and rides.loc[ride, 'first_ride'] < datetime.datetime(2016,4,4):
    cohort1.append(rides.loc[ride, 'driver_id'])
  elif rides.loc[ride, 'first_ride'] >= datetime.datetime(2016,4,4) and rides.loc[ride, 'first_ride'] < datetime.datetime(2016,4,11):
    cohort2.append(rides.loc[ride, 'driver_id'])
  elif rides.loc[ride, 'first_ride'] >= datetime.datetime(2016,4,11) and rides.loc[ride, 'first_ride'] < datetime.datetime(2016,4,18):
    cohort3.append(rides.loc[ride, 'driver_id'])
  elif rides.loc[ride, 'first_ride'] >= datetime.datetime(2016,4,18) and rides.loc[ride, 'first_ride'] < datetime.datetime(2016,4,25):
    cohort4.append(rides.loc[ride, 'driver_id'])
  elif rides.loc[ride, 'first_ride'] >= datetime.datetime(2016,4,25) and rides.loc[ride, 'first_ride'] < datetime.datetime(2016,5,2):
    cohort5.append(rides.loc[ride, 'driver_id'])   
  elif rides.loc[ride, 'first_ride'] >= datetime.datetime(2016,4,25) and rides.loc[ride, 'first_ride'] < datetime.datetime(2016,5,2):
    cohort6.append(rides.loc[ride, 'driver_id'])       
  elif rides.loc[ride, 'first_ride'] >= datetime.datetime(2016,5,2) and rides.loc[ride, 'first_ride'] < datetime.datetime(2016,5,9):
    cohort7.append(rides.loc[ride, 'driver_id'])      
  elif rides.loc[ride, 'first_ride'] >= datetime.datetime(2016,5,9) and rides.loc[ride, 'first_ride'] < datetime.datetime(2016,5,16):
    cohort8.append(rides.loc[ride, 'driver_id']) 

cohort = {'cohort 1':len(cohort1), 'cohort 2':len(cohort2), 'cohort 3':len(cohort3), 'cohort 4':len(cohort4), 'cohort 5':len(cohort5), 'cohort 6':len(cohort6), 'cohort 7':len(cohort7), 'cohort 8':len(cohort8)}
cohort_df = pd.DataFrame(data=cohort, index=[0])

#calculates active drivers remaining after a given date (if they gave a ride after specified date, they are considered still active)
def retention(cohort, date1):
  retained = 0
  ret_drivers = []
  for driver in cohort:
    timestamps = rides.at[rides[rides.driver_id == driver].index[0],'timestamp']
    timestamps = sorted(timestamps)
    for timestamp in timestamps:
      if timestamp >= date1:
        retained += 1
        ret_drivers.append(driver)
        break
  return(ret_drivers)

starting = [cohort1, cohort2, cohort3, cohort4, cohort5, cohort6, cohort7, cohort8]

#calculates retention (drivers remaining) on weekly basis
def new_week(week_num, starting):
  new_start = []
  cohort = 0
  for start in starting:
    remaining = retention(start, datetime.datetime(2016,3,28) + (cohort * timedelta(days=7)) + (week_num * timedelta(days=7)))
    new_start.append(remaining)
    cohort+=1
  return(new_start)

#fills out the cohort churn analysis table
retained_by_cohort1 = new_week(1,starting)
retained1 = []
for cohort in retained_by_cohort1:
   retained1.append(len(cohort))
cohort_df.loc[len(cohort_df)] = retained1

retained_by_cohort2 = new_week(2,retained_by_cohort1)
retained2 = []
for cohort in retained_by_cohort2:
   retained2.append(len(cohort))
cohort_df.loc[len(cohort_df)] = retained2

retained_by_cohort3 = new_week(3,retained_by_cohort2)
retained3 = []
for cohort in retained_by_cohort3:
   retained3.append(len(cohort))
cohort_df.loc[len(cohort_df)] = retained3

retained_by_cohort4 = new_week(4,retained_by_cohort3)
retained4 = []
for cohort in retained_by_cohort4:
   retained4.append(len(cohort))
cohort_df.loc[len(cohort_df)] = retained4

retained_by_cohort5 = new_week(5,retained_by_cohort4)
retained5 = []
for cohort in retained_by_cohort5:
   retained5.append(len(cohort))
cohort_df.loc[len(cohort_df)] = retained5

cohort_df = cohort_df.drop(columns='cohort 6')
cohort_df

#calculate average churn rates within cohorts and then across cohorts to get an average projected lifetime
avgs = []
all_churns = []

for cohort in cohort_df.columns:
  churns = []
  ret = list(cohort_df[cohort])
  total = 0
  for i in range(len(ret)-1):
    difference = ret[i] - ret[i+1]
    churn = difference/ret[i]
    total += churn
    churns.append(churn)
  average = total/5
  lifespan = 1/average
  print(cohort, '|', lifespan)
  avgs.append(lifespan)
  all_churns.append(churns)
  
avg_lifespan = sum(avgs)/len(avgs)
print(avg_lifespan)

#compare average churns on a week-week basis
churndf = pd.DataFrame(all_churns)
churndf.describe()

"""#calculating fares"""

#convert ride data to minutes & miles
ride_ids = ride_ids.reset_index()
ride_ids.ride_distance = ride_ids.ride_distance.apply(lambda x: x*0.000621371)
ride_ids.ride_duration = ride_ids.ride_duration.apply(lambda x: x/60)

#calculate fares for all rides
fares = []
speeds = []

for ride in ride_ids.index:
  subtotal = (2 + (ride_ids.loc[ride, 'ride_distance'] * 1.15) + (ride_ids.loc[ride, 'ride_duration'] * .22)) * (1 + (ride_ids.loc[ride, 'ride_prime_time']/100))
  fare = subtotal + 1.75 
  
  if fare < 5:
    fare = 5
  elif fare > 400:
    fare = 400
  
  speed = ride_ids.loc[ride, 'ride_distance']/ride_ids.loc[ride, 'ride_duration']
  fares.append(fare)
  speeds.append(speed)

ride_ids['fares'] = fares
ride_ids['speeds'] = speeds

"""#compiling the data"""

#combines ride info & fares with drivers & drop-off timestamps
driver_rev = ride_ids.merge(dropped_off, how='inner', on='ride_id')
driver_rev2 = driver_rev.merge(active_weeks, how='left', on ='driver_id')
driver_rev2 = pd.merge(driver_rev2,rides[['driver_id','onboarding-first']],on='driver_id', how='left')
driver_rev2['onboarding-first'] =  pd.to_timedelta(driver_rev2['onboarding-first'])/pd.Timedelta(hours=24)

#creates a dataframe of average metrics
avg_per_ride = driver_rev2.groupby('driver_id').agg({'ride_id':lambda x:list(x), 
                         'ride_distance':'mean', 
                         'ride_duration':'mean', 
                         'ride_prime_time':'mean', 
                         'fares':'mean',
                         'timestamp': lambda x: (x.max() - x.min())/pd.Timedelta(days=7),
                         'speeds': 'mean',
                         'active_weeks':'mean',
                         'onboarding-first':'mean'})
avg_per_ride = avg_per_ride.rename(columns={'ride_distance':'avg_distance/ride', 'ride_duration':'avg_duration/ride', 'ride_prime_time':'avg_primetime', 'fares':'avg_fares/ride', 'timestamp':'first-last', 'speeds':'avg_mps/ride'})

#reset driver_rev2 dataframes
driver_rev = ride_ids.merge(dropped_off, how='inner', on='ride_id')
driver_rev2 = driver_rev.merge(active_weeks, how='left', on ='driver_id')
driver_rev2 = pd.merge(driver_rev2,rides[['driver_id','onboarding-first']],on='driver_id', how='left')
driver_rev2['onboarding-first'] =  pd.to_timedelta(driver_rev2['onboarding-first'])/pd.Timedelta(hours=24)

#function to be used in agg statement 
def prime(x):
  return (x>0).sum()

#groups data by driver_id > list of ride_ids, total ride distance/duration, average primetime, lifetime thus far
driver_totals = driver_rev2.groupby('driver_id').agg({'ride_id':lambda x:list(x), 
                         'ride_distance':'sum', 
                         'ride_duration':'sum', 
                         'ride_prime_time':prime, 
                         'fares':'sum',
                         'timestamp': lambda x: ((x.max() - x.min())/pd.Timedelta(days=7)),
                         'speeds':'mean',
                         'active_weeks':'mean',
                         'onboarding-first':'mean'})
driver_totals = driver_totals.rename(columns={'ride_distance':'total_miles_driven', 'ride_duration':'total_minutes_driven', 'ride_prime_time':'rides_with_pt', 'fares':'total_fares_collected', 'timestamp':'first-last', 'speeds':'avg_mps/ride'})

#calculates various metrics

total_rides_given = []
for driver in driver_totals.index:
  rides = driver_totals.at[driver, 'ride_id']
  rides_given = len(rides)
  total_rides_given.append(rides_given)

driver_totals['total_rides_given'] = total_rides_given

driver_totals['avg_weekly_fare'] = driver_totals.total_fares_collected / driver_totals.active_weeks

driver_totals['avg_rides_per_week'] = driver_totals.total_rides_given/driver_totals.active_weeks

driver_totals['avg_hours_per_week'] = (driver_totals.total_minutes_driven/60)/driver_totals.active_weeks

driver_totals['avg_miles_per_week'] = driver_totals.total_miles_driven/driver_totals.active_weeks

#eliminate duplicate columns then merge
driver_totals = driver_totals.drop(columns = ['first-last','avg_mps/ride','active_weeks','onboarding-first'])
avg_per_ride = avg_per_ride.drop(columns=['ride_id'])
final = driver_totals.merge(avg_per_ride, how='inner', on='driver_id')

#new metrics calculated (inactive weeks, percent PT, hourly fare) and added in
inactive_weeks = []
for driver in final.index:
  inactive = final.at[driver,'first-last'] - final.at[driver,'active_weeks']
  if inactive < 1:
    inactive_weeks.append(0)
  else:
    inactive_weeks.append(inactive)
final['inactive_weeks'] = inactive_weeks
final['fares/hour'] = final.total_fares_collected/(final.total_minutes_driven/60)
final['percentPT'] = final.rides_with_pt/final.total_rides_given

#subset of final dataframe for drivers with first ride prior to 4/4
mar_first_rides = final.merge(true_lifespans, how='inner', on='driver_id')
mar_first_rides = mar_first_rides.dropna(axis=0)

consistent = final.loc[final['inactive_weeks']==0]
consistent.describe()

not_consistent = final.loc[final['inactive_weeks']>0]
not_consistent.describe()

proactive = final.loc[final['onboarding-first']<.5]
proactive.describe()

lazy = final.loc[final['onboarding-first']>=.5]
lazy.describe()

dipped = mar_first_rides[mar_first_rides.mar_lifespans < 10]
dipped.describe()

stayed = mar_first_rides[mar_first_rides.mar_lifespans >= 10]
stayed.describe()

inactive = final.loc[final['avg_rides_per_week']<25]
inactive.describe()

active = final.loc[final['avg_rides_per_week']>=25]
active.describe()

"""#other considerations"""

#analyzing on-hour drivers vs off-hour drivers
arrived_at = ride_timestamps.loc[ride_timestamps['event'] == 'arrived_at']
arrived_at = arrived_at.reset_index()
arrived_at.drop_duplicates(subset='ride_id', keep='last')

df = ride_ids.merge(arrived_at, how='inner', left_index = True, right_on = 'ride_id')
df = df.reset_index().set_index('driver_id')
df = df.groupby('driver_id').agg({'timestamp':lambda x: list(x)})
df = df.reset_index()

results = {}
for i in df.index:
    offhrs = [a for a in df.at[i,'timestamp'] if (a.hour > 18 and a.weekday() < 5) or a.weekday() >= 5 ]
    onhrs = [a for a in df.at[i, 'timestamp'] if a.hour >= 6 and a.hour <= 19 and a.weekday() < 5]
    results[df.at[i, 'driver_id']] = [len(offhrs), len(onhrs)]

job2= []
other = []
for key in results:
    if results[key][1] == 0:
        job2.append(key)
    else: 
        other.append(key)
print('people who drive as a second job:', len(job2))
print('other / uncertain:', len(other))

"""# Data **Visualization**"""

sns.scatterplot(x='percentPT', y='avg_weekly_fare', data=final, color='deeppink')
plt.xlabel('Primetime Percent')
plt.ylabel('Average Weekly Fare')
plt.title('Primetime Percent vs Average Weekly Fare')
plt.savefig('Primetime Percent.png')

group1 = final[final.percentPT < 0.3]
group2 = final[(final.percentPT >= 0.3) & (final.percentPT <= 0.5)]
group3 = final[final.percentPT > 0.5]

averages = [group1['avg_weekly_fare'].mean(), group2['avg_weekly_fare'].mean(), group3['avg_weekly_fare'].mean()]
groups = ['PT% < 30%', '30% <= PT% <= 50%', 'PT% > 50%']
data = pd.DataFrame({'groups':groups, 'averages':averages})

sns.factorplot(x='groups', y='averages', data=data, kind='bar', color='deeppink')
plt.xlabel('Primetime Percent Groups')
plt.ylabel('Average Weekly Fare')
plt.title('Average Weekly Fare by Primetime Percent')
plt.subplots_adjust(top=0.88)
plt.savefig('Primetime%Groups.png')

from scipy import stats
def r2(x,y):
    return stats.pearsonr(x,y)[0] ** 2

sns.jointplot(x='avg_hours_per_week', y='avg_weekly_fare',data=final, kind='reg', stat_func=r2, color='deeppink', 
              xlim=(0, 25), ylim=(0,1500))
plt.xlabel('Average Hours per Week')
plt.ylabel('Average Weekly Fare')
plt.savefig('Hrs vs Weekly Fare.png')

df2 = pd.DataFrame({'x':['Less than 10 Weeks', 'More than 10 Weeks'], 'y':[18.6, 25.9]})
other = sns.factorplot('x', 'y', data=df2, kind='bar', color='deeppink')
other.set_axis_labels('Driver Length (in Weeks)', 'Average Rides per Week')
other.fig.suptitle('     Average Rides per Week by Driver Length')
other.savefig('Ride_per_Week.png')
other.savefig('Avg Rides by Driver Length.png')

churndata = pd.DataFrame({'x':['0-1', '1-2', '2-3', '3-4', '4-5'], 'y':[1.8, 3.4, 3.63, 6.7, 6.24]})
churngraph = sns.lineplot('x', 'y', data=churndata, color='deeppink')
churngraph.set(xlabel='Week', ylabel='Churn Percent')
churngraph.set_title('Churn Rate Over Time')
graph = churngraph.get_figure()
graph.savefig('ChurnGraph.png')

sns.scatterplot(x='fares/hour', y='avg_weekly_fare', data=final, color='deeppink')
axes = g.axes.flatten()

plt.title('Fares per Hour vs Average Weekly Fare')
plt.xlabel('Fares per Hour')
plt.ylabel('Average Weekly Fare')
plt.savefig('Fare per Hour vs Avg Wekly Fare.png')

data = pd.DataFrame({'x':['Some Inactive Weeks', 'No Inactive Weeks'], 'y':[218.457538, 396.329802]})
sns.barplot('x', 'y', data=data, color='deeppink')
plt.xlabel('Driver Consistency Groups')
plt.ylabel('Average Weekly Fare')
plt.title('Driver Consistency')
plt.savefig('DriverConsistency.png')

data = pd.DataFrame({'x':['Regular', 'Proactive'], 'y':[356.415162, 370.340270]})
sns.barplot('x', 'y', data=data, color='deeppink')
plt.xlabel('Driver Type')
plt.ylabel('Average Weekly Fare')
plt.title('Proactivity vs Weekly Fare')
plt.savefig('ProacvsWeeklyFare.png')

data = pd.DataFrame({'x':['Regular', 'Proactive'], 'y':[6.256303, 6.592444]})
sns.barplot('x', 'y', data=data, color='deeppink')
plt.xlabel('Driver Type')
plt.ylabel('Hours Driver per Week')
plt.title('Proactivity vs Hours Driven per Week')
plt.savefig('ProacvsHrsDrivenpWeek.png')